<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom"><generator uri="http://jekyllrb.com" version="3.1.2">Jekyll</generator><link href="http://tarunjangra.com/rss.xml" rel="self" type="application/atom+xml" /><link href="http://tarunjangra.com/" rel="alternate" type="text/html" /><updated>2016-12-10T22:25:44+00:00</updated><id>http://tarunjangra.com/</id><title>Tarun Jangra</title><subtitle>Tarun Jangra</subtitle><author><name>Tarun Jangra</name><email>tarun@izap.in</email></author><entry><title>My First post with Jekyll</title><link href="http://tarunjangra.com/2016/07/09/hello-world.html" rel="alternate" type="text/html" title="My First post with Jekyll" /><published>2016-07-09T00:00:00+00:00</published><updated>2016-07-09T00:00:00+00:00</updated><id>http://tarunjangra.com/2016/07/09/hello-world</id><content type="html" xml:base="http://tarunjangra.com/2016/07/09/hello-world.html">&lt;p&gt;In this blog i do not have any thing particular to talk about. So it is just an introduction
of my new blog built on Jekyll. Since it is Jekyll based, so i&amp;#39;ve used Travis-CI for building
and github for hosting this blog.&lt;/p&gt;

&lt;p&gt;As described in &lt;a href=&quot;/about-me.html&quot;&gt;About Me&lt;/a&gt;, I&amp;#39;m passionate about programming, cloud computing,
Entrepreneurship. So that&amp;#39;s whyat i&amp;#39;ll be writing about.&lt;/p&gt;

&lt;p&gt;So just want to say &amp;quot;Hello&amp;quot; and Thank you to take time to read this blog.&lt;/p&gt;</content><category term="blogging" /><category term="tarun jangra" /><category term="jekyll" /><category term="jangra" /><summary>In this blog i do not have any thing particular to talk about. So it is just an introduction
of my new blog built on Jekyll. Since it is Jekyll based, so i&amp;#39;ve used Travis-CI for building
and github for hosting this blog.

As described in About Me, I&amp;#39;m passionate about programming, cloud computing,
Entrepreneurship. So that&amp;#39;s whyat i&amp;#39;ll be writing about.

So just want to say &amp;quot;Hello&amp;quot; and Thank you to take time to read this blog.</summary></entry><entry><title>ElasticSearch restore failed when s3-gateway is activated</title><link href="http://tarunjangra.com/2014/07/11/elasticSearch-restore-failed-when-s3-gateway-activated.html" rel="alternate" type="text/html" title="ElasticSearch restore failed when s3-gateway is activated" /><published>2014-07-11T00:00:00+00:00</published><updated>2014-07-11T00:00:00+00:00</updated><id>http://tarunjangra.com/2014/07/11/elasticSearch-restore-failed-when s3-gateway-activated</id><content type="html" xml:base="http://tarunjangra.com/2014/07/11/elasticSearch-restore-failed-when-s3-gateway-activated.html">&lt;p&gt;Hufffff, Unfortunately i met this edge case. I have recovered from this situation. Here’s my scenario.&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;I am on ElasticSearch Version 1.1.0&lt;/li&gt;
&lt;li&gt;I have two data nodes. One is primary and other is replica.&lt;/li&gt;
&lt;li&gt;I am taking regular snapshots of my indexes.&lt;/li&gt;
&lt;li&gt;I am no more taking snapshots, So I have installed s3-gateway plugin to keep updating s3 buckets for persistent indexes.&lt;/li&gt;
&lt;/ul&gt;

&lt;!--more--&gt;

&lt;p&gt;Because of bulk import, i have stopped my replica to make import little faster. Once import get completed. I felt high CPU and Memory usage. And since i was aware that my indexes are safe because i am supporting s3-gateway. So i decided to restart remaining data node. Fuck…. It was a big mistake. When i tried to restart, it was not recovering all indexes. And we were about to launch our site in next two hours. And i am left with no index.&lt;/p&gt;

&lt;p&gt;Struggling here and there, i came to know that i am suffered with Bug in ElasticSearch. I tried to follow instruction at the end of this thread where i was suppose to update/edit metadata file from s3-bucket. I did that but no luck.&lt;/p&gt;

&lt;p&gt;Problem i found, All indexes and shards suppose to have _source folders. And i had so many indexes and their shards where _source folder was missing. And those indexes were unrecoverable. I have no solutions at that place and was literately sweating in Air Conditioned Room. &lt;/p&gt;

&lt;p&gt;Then one of my colleague, Narinder Kaur has joined me. And she gave me necessary support and we tried some more messes to fix it. Since i already made a mistake, So i took one backup of existing elasticsearch so that i would be able to back at same place in case of any other mess. And solutions we were planning to try was totally crap.&lt;/p&gt;

&lt;p&gt;So, Solution we tried. and which actually works….. Wow!.&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;I updated my elasticsearch.yml, and remove s3-gateway settings related to my s3 bucket.&lt;/li&gt;
&lt;li&gt;I stopped elasticsearch.&lt;/li&gt;
&lt;li&gt;I rename my old cluster (elasticsearch) to elasticsearch.original.&lt;/li&gt;
&lt;li&gt;Restarted Elasticsearch. And it created new blank cluster. where i have no indexes.&lt;/li&gt;
&lt;li&gt;I created all required indexes with the same number of shards and replicas i previously had. In my case i had 5 indexes and 5 shards per index.&lt;/li&gt;
&lt;li&gt;Now i stop elasticsearch again.&lt;/li&gt;
&lt;li&gt;Start deleting (elasticsearch/nodes/0/indices/{index_name}&amp;gt;/{0,1,2,3,4}/{index,translog}. And move (elasticsearch.original/nodes/0/indices/{index_name}/{0,1,2,3,4}/{index,translog}) to (elasticsearch/nodes/0/indices/{index_name}/&lt;0,1,2,3,4&gt;/{index,translog})
&lt;strong&gt;Note&lt;/strong&gt;: Here, i did not touch _state folder of blank indexes. And now my all indexes has _status folder in each shard and each index.&lt;/li&gt;
&lt;li&gt;I copied all indexes as in 5th step.&lt;/li&gt;
&lt;li&gt;Restart ElasticSearch. and i found all indexes were recovered.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Observation: Well you should run your all custom mappings in blank indexes. I found some errors because i did not execute my mapping.&lt;/p&gt;

&lt;p&gt;Thank god, Now all indexes were recovered. And Thanks to Narinder Kaur, she got me required support at that time.&lt;/p&gt;</content><category term="blogging" /><category term="tarun jangra" /><category term="elastic search" /><category term="elastica" /><category term="S3-gateway" /><summary>Hufffff, Unfortunately i met this edge case. I have recovered from this situation. Here’s my scenario.


I am on ElasticSearch Version 1.1.0
I have two data nodes. One is primary and other is replica.
I am taking regular snapshots of my indexes.
I am no more taking snapshots, So I have installed s3-gateway plugin to keep updating s3 buckets for persistent indexes.</summary></entry><entry><title>How to install go-daddy ssl certificate on amazon load balancer</title><link href="http://tarunjangra.com/2012/12/29/how-to-install-godaddy-ssl-on-ELB.html" rel="alternate" type="text/html" title="How to install go-daddy ssl certificate on amazon load balancer" /><published>2012-12-29T00:00:00+00:00</published><updated>2012-12-29T00:00:00+00:00</updated><id>http://tarunjangra.com/2012/12/29/how-to-install-godaddy-ssl-on-ELB</id><content type="html" xml:base="http://tarunjangra.com/2012/12/29/how-to-install-godaddy-ssl-on-ELB.html">&lt;p&gt;I was struggling around to install SSL Certificate on ELB. And finally i’ve made that. Following are the steps you need to follow.&lt;/p&gt;

&lt;h3 id=&quot;requirements-amp-prerequisites&quot;&gt;Requirements &amp;amp; Prerequisites:&lt;/h3&gt;

&lt;ol&gt;
&lt;li&gt;Linux having openssl and apache installed.&lt;/li&gt;
&lt;li&gt;Open shell terminal on your Linux Box.&lt;/li&gt;
&lt;/ol&gt;

&lt;!--more--&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-bash&quot; data-lang=&quot;bash&quot;&gt;openssl genrsa -des3 -out private.key 1024
openssl req -new -key private.key -out www.your-web-site.com.csr&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;p&gt;You will be prompt to provide some basic information. Make sure you have added “Common Name”; a fully qualified domain name. like “www.xyz.com”&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;Open to &lt;a href=&quot;http://www.godaddy.com&quot;&gt;GoDaddy&lt;/a&gt; and go to ssl management control panel&lt;/li&gt;
&lt;li&gt;Select your Certificate. And click on Re-Key button.&lt;/li&gt;
&lt;li&gt;Copy content of “www.your-web-site.com.csr” and paste the content in “CSR” field. And press Re-Key.&lt;/li&gt;
&lt;li&gt;It will prompt you to download the keys. Available options to download are Apache, Nginx and Other. By the way, i used “Other” to download my keys to be used on ELB.&lt;/li&gt;
&lt;li&gt;Now unzip the downloaded file. It should have two *.crt files.&lt;/li&gt;
&lt;/ol&gt;

&lt;h3 id=&quot;now-back-to-your-terminal&quot;&gt;Now back to your terminal.&lt;/h3&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-bash&quot; data-lang=&quot;bash&quot;&gt;openssl rsa -in private.key -out private.pem&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;p&gt;Now you will have following files in your current location.&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;private.key&lt;/li&gt;
&lt;li&gt;private.pem&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;http://www.web-site.com.csr&quot;&gt;www.web-site.com.csr&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;sf_bundle.crt&lt;/li&gt;
&lt;li&gt;your-domain.com.crt&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Now open your load balancer console and add https support. it will prompt you to add following values.&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;Certificate Name:* -&amp;gt; Put any friendly name&lt;/li&gt;
&lt;li&gt;Private Key:* -&amp;gt; Paste content of private.pem&lt;/li&gt;
&lt;li&gt;Public Key Certificate:* -&amp;gt; Paste content of your-domain.com.crt.&lt;/li&gt;
&lt;li&gt;Certificate Chain: -&amp;gt; Paste content of sf_bundle.crt&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Once done, Save all these values and here you go.&lt;/p&gt;</content><category term="GoDaddy" /><category term="SSL" /><category term="ELB" /><category term="AWS" /><summary>I was struggling around to install SSL Certificate on ELB. And finally i’ve made that. Following are the steps you need to follow.

Requirements &amp;amp; Prerequisites:


Linux having openssl and apache installed.
Open shell terminal on your Linux Box.</summary></entry><entry><title>Our development workflow with gitflow</title><link href="http://tarunjangra.com/2012/01/19/our-development-workflow-with-git.html" rel="alternate" type="text/html" title="Our development workflow with gitflow" /><published>2012-01-19T00:00:00+00:00</published><updated>2012-01-19T00:00:00+00:00</updated><id>http://tarunjangra.com/2012/01/19/our-development-workflow-with-git</id><content type="html" xml:base="http://tarunjangra.com/2012/01/19/our-development-workflow-with-git.html">&lt;p&gt;We are using git since 2009. Recently we have been forced by a platform to implement better development workflow. Where we 
handle better branching, code releases etc. And we found gitflow, A collection of git extensions provide high level of git 
based operations. &amp;lt;!--more--&amp;gt;I found it pretty much worthy to share our experience. Earlier than gitflow, we were using git with Master 
branch only where all developers suppose to push and code is suppose to move to development server and after testing, 
it is suppose to deploy on production server. Which is bit cumbersom process. and as we are getting in the requirement of better 
tracked development with less efforts we start feeling to have some serious process to get in. We have followed Vincent Driessen&amp;#39;s 
branching model.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;http://tarunjangra.com/images/assets/git-workflow-gitflow.png&quot; alt=&quot;Gitflow&quot;&gt;&lt;/p&gt;

&lt;p&gt;Master branch will be now our production ready branch. And Development branch will be our dev server branch. These two branches 
are suppose to be in the system for infinite time. We have learnt to keep  some temporary branches like “Feature branches” and 
“Release branches” which will really play a great role in the architecture we are workingin. We are using “Pivotal Tracker” for 
our Agile methodology, So when we have new milestone with multiple stories for a particular feature. It means, developer 
need to create new branche with the name “Feature/&lt;feature-name&gt;“. This branch is suppose to be cloned from master branch 
and suppose to be in the system till the completion of the feature. And then merge back to master branch. So in the whole 
release we are suppose to complete all pivotal stories by story ids.&lt;/p&gt;

&lt;p&gt;I am looking for some automatic process where all stories get started when developer creates the Feature branch. And when 
he deliver the whole feature and merge the branch back to the development. It should automatically change the status of the 
story to be “Delivered”. QA team will test and either accept or reject the corresponding story. I know webhooks provided
by github.com which can be implemented to achieve this with pivotal tracker.&lt;/p&gt;

&lt;p&gt;Overall, Gitflow methodology make the development flow quite better then what we were doing ealrier.&lt;/p&gt;</content><category term="GIT" /><category term="Collaboration" /><category term="Agile" /><summary>We are using git since 2009. Recently we have been forced by a platform to implement better development workflow. Where we 
handle better branching, code releases etc. And we found gitflow, A collection of git extensions provide high level of git 
based operations.</summary></entry><entry><title>Round-robin at application level to Balance MySQL Database Load</title><link href="http://tarunjangra.com/2011/06/10/Round-robin-at-application-level-to-Balance-MySQL-Database-Load.html" rel="alternate" type="text/html" title="Round-robin at application level to Balance MySQL Database Load" /><published>2011-06-10T00:00:00+00:00</published><updated>2011-06-10T00:00:00+00:00</updated><id>http://tarunjangra.com/2011/06/10/Round-robin-at-application-level-to-Balance-MySQL-Database-Load</id><content type="html" xml:base="http://tarunjangra.com/2011/06/10/Round-robin-at-application-level-to-Balance-MySQL-Database-Load.html">&lt;p&gt;Round robin technique facilitates you to distribute your read queries on number of available resources even if all servers are located
at different locations. Huge traffic sites like Facebook has to has such techniques working at the background to serve as fast as
 possible. I would like to discuss one of my personal implementation experience for such a large potential social networking site.
  Cloud computing is really help full but it also needs logical approach at programming level.&lt;/p&gt;

&lt;h3 id=&quot;approach-1-six-servers-architecture-on-amazon-cloud&quot;&gt;Approach 1: Six servers architecture on amazon cloud.&lt;/h3&gt;

&lt;p&gt;WOW! I had implemented 1 load balancer, 1 mysql master db, 1 mysql slave db and 3 application server. Such an architecture&lt;br&gt;
can handle huge traffic. Since there is a separate application server layer where we can add more application servers anytime 
we need. So user requests get balanced on 3 application servers and they get response. But in my application i had one more 
problem. When user click on single link it executes 100+ SQLs because there is a framework overhead and some intentional queries.
Hmmmm, So MySql load is never balanced with this technique and it has to be. Because 1 request is triggering 100+ SQLs.
So i drill down to find out the solution and decided to separate sql reads and writes. Ok so with this, i get an opportunity 
to divide separate Writes of MySQL db and initiated one mysql slave server.&lt;/p&gt;

&lt;h3 id=&quot;does-this-really-get-me-at-the-end-of-performance-level&quot;&gt;Does this really get me at the end of performance level?&lt;/h3&gt;

&lt;p&gt;No. Because we use read queires more frequently then write. Son in 100+ SQLs i have lesser database writes. So My write server 
is still have idle resources.&lt;/p&gt;

&lt;h3 id=&quot;here-is-where-round-robin-comes-in&quot;&gt;Here is where Round Robin comes in.&lt;/h3&gt;

&lt;p&gt;If i could be able to develop a logic which distributes my 100+ SQLs to any number of replicated instances available. 
That could really work for me. Say i have 5 read servers for 100+ SQL. Than i can distribute around 20 SQL per server per request. 
And as we increase number of read server. System can adjust it self to distribute (SQL queries) / (Number of servers) (Qn / Sn). 
In this way, all of my server will work for every SQL requested from the system. And I could get maximum performance from servers.
 Because there is no use if we have 1000 Servers and 1 server is responding for 1 complete request. Because in this case 999 
 servers are free and which is wastage of Money. So i implemented that in My PHP application and that really makes sense to be 
 available on Cloud to use maximum resources.&lt;/p&gt;</content><category term="RDBMD" /><category term="MySQL" /><category term="Round robin" /><category term="AWS" /><category term="RDS" /><summary>Round robin technique facilitates you to distribute your read queries on number of available resources even if all servers are located
at different locations. Huge traffic sites like Facebook has to has such techniques working at the background to serve as fast as
 possible. I would like to discuss one of my personal implementation experience for such a large potential social networking site.
  Cloud computing is really help full but it also needs logical approach at programming level.

Approach 1: Six servers architecture on amazon cloud.

WOW! I had implemented 1 load balancer, 1 mysql master db, 1 mysql slave db and 3 application server. Such an architecture
can handle huge traffic. Since there is a separate application server layer where we can add more application servers anytime 
we need. So user requests get balanced on 3 application servers and they get response. But in my application i had one more 
problem. When user click on single link it executes 100+ SQLs because there is a framework overhead and some intentional queries.
Hmmmm, So MySql load is never balanced with this technique and it has to be. Because 1 request is triggering 100+ SQLs.
So i drill down to find out the solution and decided to separate sql reads and writes. Ok so with this, i get an opportunity 
to divide separate Writes of MySQL db and initiated one mysql slave server.

Does this really get me at the end of performance level?

No. Because we use read queires more frequently then write. Son in 100+ SQLs i have lesser database writes. So My write server 
is still have idle resources.

Here is where Round Robin comes in.

If i could be able to develop a logic which distributes my 100+ SQLs to any number of replicated instances available. 
That could really work for me. Say i have 5 read servers for 100+ SQL. Than i can distribute around 20 SQL per server per request. 
And as we increase number of read server. System can adjust it self to distribute (SQL queries) / (Number of servers) (Qn / Sn). 
In this way, all of my server will work for every SQL requested from the system. And I could get maximum performance from servers.
 Because there is no use if we have 1000 Servers and 1 server is responding for 1 complete request. Because in this case 999 
 servers are free and which is wastage of Money. So i implemented that in My PHP application and that really makes sense to be 
 available on Cloud to use maximum resources.</summary></entry></feed>
